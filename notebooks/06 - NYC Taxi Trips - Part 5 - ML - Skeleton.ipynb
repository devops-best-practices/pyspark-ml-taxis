{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Machine Learning\n",
    "\n",
    "Finally we want to apply some machine learning to the taxi data. We will try to build a model for answering the following question:\n",
    "\n",
    "    As a Taxi driver, what time and location would be best on a specific day to make most money.\n",
    "    \n",
    "This is a relevant question in order to help maximizing a Taxi drivers income. We will try to build a model which predicts the total fare amount for each date and hour and each (grid) location. This way the taxi driver can predict the expected total amount of money being made at a specific point in time at a specific location, so he can better plan his shifts.\n",
    "\n",
    "Of course given the data that we currently have, we cannot precisely give an answer, because even locations with a low total fare amount may be attractive, as long as there are only very few taxi cabs. But we do not know how many taxi cabs were simply waiting or were driving around without passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_basedir = \"file:///srv/jupyter/nyc-dwh\"\n",
    "integrated_basedir = dwh_basedir + \"/integrated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Setup Environment\n",
    "\n",
    "Before we begin, we create a local Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read Taxi Data\n",
    "\n",
    "Now we can read in the taxi data from the structured zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_taxi_trips = spark.read.parquet(integrated_basedir + \"/taxi-trips-hourly\")\n",
    "hourly_taxi_trips.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_taxi_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Split Training and Validation set\n",
    "\n",
    "As a first step, we split up the whole data set into a training and a validation data set. Typical data sets are split randomly, but for time series data sets a non-random split is preferrable in order to avoid an undesired information creep from future observations. Therefore we create a split filtering by date, such that about 80% of records are used for training and the remaining 20% of all records will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 1 - training_fraction\n",
    "split_date = datetime.date(2013, 1, 1) + datetime.timedelta(days=training_fraction*(365))\n",
    "print(\"split_date=\\\"\" + str(split_date) + \"\\\"\")\n",
    "\n",
    "# Pickup all records with a date lower than the split date\n",
    "training_data = # YOUR CODE HERE\n",
    "# Pickup all records with a date grater or equal than the split date\n",
    "validation_data = # YOUR CODE HERE\n",
    "\n",
    "# Count the number of records for both the training and the validation data set\n",
    "training_data_count = # YOUR CODE HERE\n",
    "validation_data_count = # YOUR CODE HERE\n",
    "\n",
    "print(\"training_data count = \" + str(training_data_count))\n",
    "print(\"validation_data count = \" + str(validation_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Features\n",
    "\n",
    "As a first step we need to create so called *features* from the training data set. Most PySpark ML algorithms expect two specific input columns: A so called *label* column containing the true value and a so called *features* column containing a vector of all variables used for prediction. \n",
    "\n",
    "The label column has to be a simple numeric value, in our case it will be the *total amount*.  The features column needs to contain the special data type *vector*, which is constructed from various attributes of our observations. Some of these attributes can be taken directly from our training data set, while other columns also need to be derived from the original values.\n",
    "\n",
    "### Feature Engineering Building Blocks\n",
    "PySpark provides lots of different feature engineering algorithms as building blocks. These building blocks are simple (or complex) transformations which typically will add new derived columns to a data frame. We will see how we can chain multiple of these components together into a so called *pipeline* later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Transformer\n",
    "PySpark provides a very generic building block for transformation which simply executes some SQL. For example for creating a combined geo location inside a single column, we can use the following SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_location_transformer = # YOUR CODE HERE\n",
    "\n",
    "training_data_1 = geo_location_transformer.transform(training_data)\n",
    "training_data_1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "One important case where the original values cannot be used directly is categorial data. For example the geo location cannot be used as a numerical value. Therefore we need a transformation which creates numerical values from this categorial feature. PySpark provides the pair of a *string indexer* followed by *one hot encoding* to create a separate multidimensional vector for each categorial variable. The pattern is always the same:\n",
    "\n",
    "```\n",
    "categorial data => StringIndexer => OneHotEncoder => vector\n",
    "```\n",
    "\n",
    "Specifically the code for the geo location looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create an index into all geo locations\n",
    "geo_indexer = # YOUR CODE HERE\n",
    "geo_index_model = # YOUR CODE HERE\n",
    "training_data_2 = # YOUR CODE HERE\n",
    "\n",
    "# Display some records\n",
    "training_data_2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now one-hot encode the generated index value\n",
    "geo_encoder = # YOUR CODE HERE\n",
    "geo_encoder_model = # YOUR CODE HERE\n",
    "training_data_3 = # YOUR CODE HERE\n",
    "\n",
    "# Display some records\n",
    "training_data_2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembler\n",
    "\n",
    "As already noted at the beginning of this section, PySpark requires all features to be available in a single column. This can be achieved by using a *vector assembler*, which will glue together all specified numerical and vector columns into a single vector column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = # YOUR CODE HERE\n",
    "\n",
    "training_data_4 = # YOUR CODE HERE\n",
    "\n",
    "training_data_4.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Now we have met all relevant building blocks. Instead of manually chaining these transformations together, you always should use a *pipeline*, where you can simply specify all transformations to apply. The pipeline also takes care of performing any `fit` phase of some transformers (like `StringIndexer` or `OneHotEncoderEstimator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "feature_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        SQLTransformer(\n",
    "            statement=\"\"\"\n",
    "                SELECT\n",
    "                    total_amount,\n",
    "                    date,\n",
    "                    hour,\n",
    "                    daily_temperature,\n",
    "                    hourly_temperature,\n",
    "                    daily_precipitation,\n",
    "                    hourly_precipitation,\n",
    "                    daily_wind_speed,\n",
    "                    hourly_wind_speed,\n",
    "                    month(`date`) - 1 AS `month_idx`,\n",
    "                    dayofweek(`date`) - 1 AS `weekday_idx`,\n",
    "                    CASE\n",
    "                        WHEN lat_idx IS NULL OR lat_idx < 0 THEN NULL\n",
    "                        WHEN long_idx IS NULL  OR long_idx < 0 THEN NULL\n",
    "                        ELSE concat(lat_idx, \"/\", long_idx) \n",
    "                    END AS geo_location,\n",
    "                    CASE WHEN\n",
    "                        bank_holiday = true THEN 1\n",
    "                        ELSE 0\n",
    "                    END AS bank_holiday\n",
    "                FROM __THIS__\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # Create one hot encoded geo location via StringIndexer and OneHotEncoderEstimator\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Create one hot encoded hour via OneHotEncoderEstimator\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Create one hot encoded weekday via OneHotEncoderEstimator\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Assembler the following columns\n",
    "        #  - one hot encoded weekday\n",
    "        #  - one hot encoded hour\n",
    "        #  - bank holiday\n",
    "        #  - one hot encoded geo location\n",
    "        #  - daily temperature\n",
    "        #  - hourly temperature\n",
    "        #  - daily precipitation\n",
    "        #  - hourly precipitation\n",
    "        #  - daily wind speed\n",
    "        #  - hourly wind speed\n",
    "        VectorAssembler(\n",
    "            handleInvalid=\"skip\",\n",
    "            inputCols=[\n",
    "                # YOUR CODE HERE\n",
    "            ],\n",
    "            outputCol='features'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature model now contains all stages and can be used as a transformer. Let's transform and display the training data as a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training_data = # YOUR CODE HERE\n",
    "features_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we specified `skip` as the strategy to handle invalid records in the vector assembler, let us check how many records where dropped from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_count = feature_model.transform(training_data).count()\n",
    "\n",
    "print(\"feature_data_count = \" + str(features_data_count))\n",
    "print(\"training_data_count = \" + str(training_data_count))\n",
    "print(\"skipped records = \" + str(training_data_count - features_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model\n",
    "\n",
    "So far we have only prepared the data by adding a feature column containg some information which should be used as independant variables for prediction. Now we finally want to train a model which makes use of these features and predict the total amount for every date, hour and geo location.\n",
    "\n",
    "We create another pipeline, which contains the feature pipeline as its first entry and a simple linear regression as its second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pipeline = Pipeline(stages=[\n",
    "    feature_pipeline,\n",
    "    # YOUR CODE HERE\n",
    "])\n",
    "\n",
    "pred_model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Prediction\n",
    "\n",
    "The `pred_model` now contains all feature transformation steps of the feature pipeline and a linear model. We can directly use this model for performing predictions of the total amount.\n",
    "\n",
    "Now we use the validation data for prediction, which we set aside at the beginning and which could not influence the training phase in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to display some columns of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data\\\n",
    "    .orderBy(\"date\", \"hour\") \\\n",
    "    .select(\n",
    "        \"date\", \"hour\",\n",
    "        \"geo_location\",\n",
    "        \"total_amount\",\n",
    "        \"pred_total_amount\"\n",
    "    ) \\\n",
    "    .limit(10)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Validation\n",
    "\n",
    "When looking at the predictions and comparing them with the true total amounts, we already suspect that our model does not perform very well. Now we want to quantify the goodness of fit using an appropriate evaluator. Note that evaluation should always be performed with the validation data, since we are not interested very much into the question how well a model describes the training data, but instead we want to understand how well the model performs with new data, which was not used during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "evaluator = # YOUR CODE HERE\n",
    "\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently our model is not very good, since its RMSE is about twice as large as the average value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Baseline Model\n",
    "\n",
    "We already saw that the model is not very good. But what can we expect? It is always helpful to come up with a very simple base line model, and every true model should better beat the base line model. In our case, we simply use the average total amount as a constant base line model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_total_amount = # YOUR CODE HERE\n",
    "baseline_validation_data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the base line model, we can now calculate the RMSE of this model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(baseline_validation_data)\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Improve Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Integrate information from the past\n",
    "\n",
    "Depending on the scenario, it can be completely legal to use data from the past as an additional feature. In this example, we assume that we can use the total amount for exactly the same hour of day and for the same location from exactly one week ago as an additional feature. This implies that we assume that this number is available at the time when new predictions are made.\n",
    "\n",
    "In other scenarios, the minimum amount of time to go back into the past, may be much larger or smaller. The importat question here is always what data is available when a new prediciton is performed.\n",
    "\n",
    "One important aspect of this approach is that no data may be available for some locations. But since our algorithms always require that all observations contain valid numbers, we fill the overall average of the metric over all locations for a specific date and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall average per date and hour to fill in missing values\n",
    "hourly_taxi_trips_avg = hourly_taxi_trips \\\n",
    "    .groupBy(\"date\", \"hour\") \\\n",
    "    .agg(\n",
    "        f.avg(\"fare_amount\").alias(\"fare_amount\"),\n",
    "        f.avg(\"tip_amount\").alias(\"tip_amount\"),\n",
    "        f.avg(\"total_amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "# Extend the incoming records by the total amount for the same location seven days ago. If no data is available for the specific location,\n",
    "# we use the overall average instead. This will require three steps:\n",
    "#  1. Join hourly taxi trips against itself, but delayed by seven days\n",
    "#  2. Join average values delayed by seven days\n",
    "#  3. Pick ether the value of the location or otherwise the average\n",
    "#\n",
    "hourly_taxi_trips_ext = hourly_taxi_trips \\\n",
    "    .alias(\"now\") \\\n",
    "    .join(\n",
    "        # Specify the data frame for self join and provide an alias\n",
    "        hourly_taxi_trips.alias(\"last_week\"), \n",
    "        # The join should be performed using date and hour and the geo location\n",
    "        (f.date_sub(f.col(\"now.date\"), 7) == f.col(\"last_week.date\")) &\n",
    "        (f.col(\"now.hour\") == f.col(\"last_week.hour\")) &\n",
    "        (f.col(\"now.lat_idx\") == f.col(\"last_week.lat_idx\")) &\n",
    "        (f.col(\"now.long_idx\") == f.col(\"last_week.long_idx\")),\n",
    "        how=\"leftOuter\"\n",
    "    )\\\n",
    "    .join(\n",
    "        # Specify the average data frame\n",
    "        hourly_taxi_trips_avg.alias(\"avg\"),\n",
    "        # The join should be performed on date and hour only\n",
    "        (f.date_sub(f.col(\"now.date\"), 7) == f.col(\"avg.date\")) &\n",
    "        (f.col(\"now.hour\") == f.col(\"avg.hour\")),\n",
    "        how=\"leftOuter\"\n",
    "    )\\\n",
    "    .select(\n",
    "        f.col(\"now.*\"),\n",
    "        f.coalesce(f.col(\"last_week.fare_amount\"), f.col(\"avg.fare_amount\")).alias(\"prev_fare_amount\"),\n",
    "        f.coalesce(f.col(\"last_week.tip_amount\"), f.col(\"avg.tip_amount\")).alias(\"prev_tip_amount\"),\n",
    "        f.coalesce(f.col(\"last_week.total_amount\"), f.col(\"avg.total_amount\")).alias(\"prev_total_amount\")\n",
    "    )\\\n",
    "    .filter(f.col(\"prev_fare_amount\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_taxi_trips_ext.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Split Training and Validation set\n",
    "\n",
    "As a first step, we split up the whole data set into a training and a validation data set. Typical data sets are split randomly, but for time series data sets a non-random split is preferrable in order to avoid an undesired information creep from future observations. Therefore we create a split filtering by date, such that about 80% of records are used for training and the remaining 20% of all records will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 1 - training_fraction\n",
    "split_date = datetime.date(2013, 1, 7) + datetime.timedelta(days=training_fraction*(365-7))\n",
    "print(\"split_date=\\\"\" + str(split_date) + \"\\\"\")\n",
    "\n",
    "training_data = hourly_taxi_trips_ext.filter(f.col(\"date\") < split_date)\n",
    "validation_data = hourly_taxi_trips_ext.filter(f.col(\"date\") >= split_date)\n",
    "\n",
    "training_data_count = training_data.count()\n",
    "validation_data_count = validation_data.count()\n",
    "\n",
    "print(\"training_data count = \" + str(training_data_count))\n",
    "print(\"validation_data count = \" + str(validation_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create Features and Train Model\n",
    "\n",
    "Using building blocks of the PySpark ML package, we create a machine learning pipeline with all feature engineering steps and the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing\n",
    "\n",
    "For some numerical features (like temperature and wind speed), it may be more appropriate to model them as categorical features. This can be done by *bucketing* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Buckets\n",
    "bucketizer = # YOUR CODE HERE\n",
    "training_data_1 = # YOUR CODE HERE\n",
    "\n",
    "training_data_1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encode buckets\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[\"daily_temperature_bucket\"],\n",
    "    outputCols=[\"daily_temperature_onehot\"]\n",
    ")\n",
    "encoder_model = encoder.fit(training_data_1)\n",
    "training_data_2 = encoder_model.transform(training_data_1)\n",
    "\n",
    "training_data_2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Now we can create a more extensive pipeline, which makes use of more features and which also performs bucketing of the weather data.\n",
    "\n",
    "In particulat the pipeline performs the following steps:\n",
    "* one hot encode geo location\n",
    "* one hot encode hour\n",
    "* one hot encode day of week\n",
    "* bucketize all weather measurements\n",
    "* perform regression\n",
    "* truncate predictions to zero from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        SQLTransformer(\n",
    "            statement=\"\"\"\n",
    "                SELECT\n",
    "                    total_amount,\n",
    "                    prev_total_amount,\n",
    "                    date,\n",
    "                    hour,\n",
    "                    daily_temperature,\n",
    "                    hourly_temperature,\n",
    "                    daily_precipitation,\n",
    "                    hourly_precipitation,\n",
    "                    daily_wind_speed,\n",
    "                    hourly_wind_speed,\n",
    "                    CASE WHEN\n",
    "                        bank_holiday = true THEN 0\n",
    "                        ELSE dayofweek(`date`)\n",
    "                    END AS weekday_idx,\n",
    "                    CASE\n",
    "                        WHEN lat_idx IS NULL OR lat_idx < 0 THEN NULL\n",
    "                        WHEN long_idx IS NULL  OR long_idx < 0 THEN NULL\n",
    "                        ELSE concat(lat_idx, \"/\", long_idx) \n",
    "                    END AS geo_location\n",
    "                FROM __THIS__\n",
    "            \"\"\"\n",
    "        ),\n",
    "        StringIndexer(\n",
    "            inputCol=\"geo_location\",\n",
    "            outputCol=\"geo_location_idx\",\n",
    "            handleInvalid=\"keep\"\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"geo_location_idx\"],\n",
    "            outputCols=[\"geo_location_onehot\"]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hour\"],\n",
    "            outputCols=[\"hour_onehot\"]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"weekday_idx\"],\n",
    "            outputCols=[\"weekday_onehot\"]\n",
    "        ),\n",
    "        \n",
    "        Bucketizer(\n",
    "            inputCol=\"daily_temperature\",\n",
    "            outputCol=\"daily_temperature_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),-10,0,10,20,30,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"daily_temperature_bucket\"],\n",
    "            outputCols=[\"daily_temperature_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"hourly_temperature\",\n",
    "            outputCol=\"hourly_temperature_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),-10,0,10,20,30,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hourly_temperature_bucket\"],\n",
    "            outputCols=[\"hourly_temperature_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"daily_precipitation\",\n",
    "            outputCol=\"daily_precipitation_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,100,200,300,400,500,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"daily_precipitation_bucket\"],\n",
    "            outputCols=[\"daily_precipitation_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"hourly_precipitation\",\n",
    "            outputCol=\"hourly_precipitation_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,50,100,150,200,250,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hourly_precipitation_bucket\"],\n",
    "            outputCols=[\"hourly_precipitation_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"daily_wind_speed\",\n",
    "            outputCol=\"daily_wind_speed_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,1,2,3,4,5,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"daily_wind_speed_bucket\"],\n",
    "            outputCols=[\"daily_wind_speed_onehot\"]\n",
    "        ),\n",
    "        Bucketizer(\n",
    "            inputCol=\"hourly_wind_speed\",\n",
    "            outputCol=\"hourly_wind_speed_bucket\",\n",
    "            handleInvalid=\"keep\",\n",
    "            splits=[-float(\"inf\"),0,1,2,3,4,5,float(\"inf\")]\n",
    "        ),\n",
    "        OneHotEncoderEstimator(\n",
    "            inputCols=[\"hourly_wind_speed_bucket\"],\n",
    "            outputCols=[\"hourly_wind_speed_onehot\"]\n",
    "        ),\n",
    "        \n",
    "        # Linear Prediction\n",
    "        VectorAssembler(\n",
    "            handleInvalid=\"skip\",\n",
    "            inputCols=[\n",
    "                'prev_total_amount',\n",
    "                'weekday_onehot',\n",
    "                'hour_onehot',\n",
    "                'geo_location_onehot',\n",
    "                'daily_temperature_onehot',\n",
    "                'hourly_temperature_onehot',\n",
    "                'daily_precipitation_onehot',\n",
    "                'hourly_precipitation_onehot',\n",
    "                'daily_wind_speed_onehot',\n",
    "                'hourly_wind_speed_onehot'\n",
    "            ],\n",
    "            outputCol='features'\n",
    "        ),\n",
    "        LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"total_amount\",\n",
    "            predictionCol=\"pred_total_amount\"\n",
    "        ),\n",
    "        SQLTransformer(\n",
    "            statement=\"\"\"\n",
    "                SELECT\n",
    "                    date,\n",
    "                    hour,\n",
    "                    geo_location,\n",
    "                    total_amount,\n",
    "                    CASE \n",
    "                        WHEN pred_total_amount > 0 THEN pred_total_amount\n",
    "                        ELSE 0\n",
    "                    END AS pred_total_amount\n",
    "                FROM __THIS__\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pred_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let us check how many training records have been dropped during the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_count = pred_model.transform(training_data).count()\n",
    "\n",
    "print(\"training_data_count = \" + str(training_data_count))\n",
    "print(\"feature_data_count = \" + str(features_data_count))\n",
    "print(\"skipped records = \" + str(training_data_count - features_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data = pred_model.transform(validation_data)\n",
    "pred_validation_data\\\n",
    "    .orderBy(\"date\", \"hour\") \\\n",
    "    .select(\n",
    "        \"date\", \"hour\",\n",
    "        \"geo_location\",\n",
    "        \"total_amount\",\n",
    "        \"pred_total_amount\"\n",
    "    ) \\\n",
    "    .limit(10)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol = \"total_amount\",\n",
    "    predictionCol = \"pred_total_amount\",\n",
    "    metricName = \"rmse\"\n",
    ")\n",
    "\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Baseline Model\n",
    "\n",
    "In order to make sense of the number, we use a simple base line model as comparison again. This time we simply predict the total amount by using the previous total amount from seven days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_validation_data = # YOUR CODE HERE\n",
    "\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"rmse = \" + str(rmse))\n",
    "print(\"real_avg = \" + str(validation_data.select(f.avg(\"total_amount\")).first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Best Time and Location\n",
    "\n",
    "Although the predicted values are not really satisfying so far, they are good enough for deciding when to make most money. In order to underline this claim, let us compare the top ten hours and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data.filter(\"date='2013-11-01'\") \\\n",
    "    .select(\"date\", \"hour\", \"geo_location\", \"total_amount\", \"pred_total_amount\") \\\n",
    "    .orderBy(f.desc(\"total_amount\")) \\\n",
    "    .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation_data.filter(\"date='2013-11-01'\") \\\n",
    "    .select(\"date\", \"hour\", \"geo_location\", \"total_amount\", \"pred_total_amount\") \\\n",
    "    .orderBy(f.desc(\"pred_total_amount\")) \\\n",
    "    .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Top 10 Recommendations\n",
    "\n",
    "Let us find out how good the recommendatations of our algorithm would be. We pick the ten best hour-locations for each day from the real data, the predicted data and the baseline model. For each selection, we also compute the *real total revenue* for all these location-hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real 10 best location-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "real_best_locations = pred_validation_data \\\n",
    "    .select(\n",
    "        f.col(\"date\"),\n",
    "        f.col(\"hour\"),\n",
    "        f.col(\"total_amount\"),\n",
    "        f.col(\"geo_location\"),\n",
    "        f.row_number().over(Window.partitionBy(\"date\").orderBy(f.col(\"total_amount\").desc())).alias(\"row_number\")\n",
    "    ) \\\n",
    "    .filter(f.col(\"row_number\") < 10)\n",
    "\n",
    "real_best_locations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_top10_totals = # YOUR CODE HERE\n",
    "\n",
    "print(\"real_top10_totals = \" + str(real_top10_totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted 10 best location-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_locations = pred_validation_data \\\n",
    "    .select(\n",
    "        f.col(\"date\"),\n",
    "        f.col(\"hour\"),\n",
    "        f.col(\"total_amount\"),\n",
    "        f.col(\"geo_location\"),\n",
    "        f.row_number().over(Window.partitionBy(\"date\").orderBy(f.col(\"pred_total_amount\").desc())).alias(\"row_number\")\n",
    "    ) \\\n",
    "    .filter(f.col(\"row_number\") < 10)\n",
    "\n",
    "pred_best_locations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_top10_totals = pred_best_locations.select(\n",
    "    f.sum(f.col(\"total_amount\"))\n",
    ").first()[0] \n",
    "\n",
    "print(\"pred_top10_totals = \" + str(pred_top10_totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 10 best location-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_best_locations = validation_data \\\n",
    "    .select(\n",
    "        f.col(\"date\"),\n",
    "        f.col(\"hour\"),\n",
    "        f.col(\"total_amount\"),\n",
    "        f.concat(f.col(\"lat_idx\"), f.lit(\"/\"), f.col(\"long_idx\")).alias(\"geo_location\"),\n",
    "        f.row_number().over(Window.partitionBy(\"date\").orderBy(f.col(\"prev_total_amount\").desc())).alias(\"row_number\")\n",
    "    ) \\\n",
    "    .filter(f.col(\"row_number\") < 10)\n",
    "\n",
    "baseline_best_locations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_top10_totals = baseline_best_locations.select(\n",
    "    f.sum(f.col(\"total_amount\"))\n",
    ").first()[0] \n",
    "\n",
    "print(\"baseline_top10_totals = \" + str(baseline_top10_totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
