{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Integrate Multiple Sources\n",
    "\n",
    "In this notebook we will integrate the preaggregated NYC taxi trips data with the weather data and with the holiday data. This will give us a data set rich of additional features which can be used for the final machine learning task.\n",
    "\n",
    "The enriched data containing information from multiple independent sources (taxi trips, weather and holidays) will be stored into the *integrated zone*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_basedir = \"/user/hadoop/nyc-dwh\"\n",
    "structured_basedir = dwh_basedir + \"/structured\"\n",
    "refined_basedir = dwh_basedir + \"/refined\"\n",
    "integrated_basedir = dwh_basedir + \"/integrated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Geopandas and friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Helper function to fetch background map tiles\n",
    "def add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n",
    "    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n",
    "    # restore original x/y limits\n",
    "    ax.axis((xmin, xmax, ymin, ymax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Taxi Data\n",
    "\n",
    "Now we can read in the hourly preaggregated taxi data from the refined zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_aggregates = spark.read.parquet(refined_basedir + \"/taxi-trips-hourly\")\n",
    "taxi_aggregates.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 NYC Taxi Trip location\n",
    "\n",
    "We now also load the taxi trips from the refined zone containing individual records per taxi trip. We use this data to calculate the average geo location of all taxi trips. To clean up the data, we reuse the quantiles previously calculated of the geo locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% quantiles of the pickup geo location, as calculated in the previous notebook\n",
    "min_pickup_longitude=-74.007629\n",
    "max_pickup_longitude=-73.77668\n",
    "min_pickup_latitude=40.705612\n",
    "max_pickup_latitude=40.840221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the average pickup geo location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips = spark.read.parquet(refined_basedir + \"/taxi-trip\")\n",
    "\n",
    "# Calculate average pickup location. The result should be stored in two columns avg_pickup_longitude and avg_pickup_latitude\n",
    "result = taxi_trips\\\n",
    "    .filter((taxi_trips[\"pickup_longitude\"] > min_pickup_longitude) & (taxi_trips[\"pickup_longitude\"] < max_pickup_longitude)) \\\n",
    "    .filter((taxi_trips[\"pickup_latitude\"] > min_pickup_latitude) & (taxi_trips[\"pickup_latitude\"] < max_pickup_latitude)) \\\n",
    "    .select(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "\n",
    "# Extract numerical values from single-record DataFrame\n",
    "first_result = result.first()\n",
    "avg_pickup_longitude = first_result[\"avg_pickup_longitude\"]\n",
    "avg_pickup_latitude = first_result[\"avg_pickup_latitude\"]\n",
    "\n",
    "print(\"avg_pickup_latitude=\" + str(avg_pickup_latitude))\n",
    "print(\"avg_pickup_longitude=\" + str(avg_pickup_longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weather Data\n",
    "\n",
    "Now load in the preaggregated hourly and daily weather data from 2013. We will try to find the weather station nearest to the average pickup location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather = spark.read.parquet(refined_basedir + \"/weather-hourly/2013\")\n",
    "daily_weather = spark.read.parquet(refined_basedir + \"/weather-daily/2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Station Master Data\n",
    "\n",
    "In order to find an appropriate weather station (which will be used for all taxi trips, since we only analyze data from NYC), we use the weather station master data, which also contains the geo location of every weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stations = spark.read.parquet(structured_basedir + \"/weather-stations\")\n",
    "weather_stations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Find Corresponding Weather Station\n",
    "\n",
    "Using the master data of all weather stations, we now try to find a station which is near to the center of all taxi trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate distance of every weather station to the average pickup location\n",
    "weather_stations_with_distance = weather_stations\\\n",
    "    .filter((weather_stations[\"BEGIN\"] <= \"20130101\") & ((weather_stations[\"END\"] >= \"20131231\") | weather_stations[\"END\"].isNull())) \\\n",
    "    .filter(weather_stations[\"WBAN\"] != \"99999\") \\\n",
    "    .select(\n",
    "        \"*\",\n",
    "        (f.pow(avg_pickup_longitude - weather_stations[\"LON\"],2) + f.pow(avg_pickup_latitude - weather_stations[\"LAT\"],2)).alias(\"geo_distance\")\n",
    "    )\n",
    "\n",
    "# Step 2: Pick nearest station by sorting the result by distance and the pick the first record\n",
    "nyc_station = # YOUR CODE HERE\n",
    "\n",
    "# Extract relevant information for later\n",
    "nyc_station_usaf = nyc_station[\"USAF\"]\n",
    "nyc_station_wban = nyc_station[\"WBAN\"]\n",
    "nyc_station_longitude = float(nyc_station[\"LON\"])\n",
    "nyc_station_latitude = float(nyc_station[\"LAT\"])\n",
    "\n",
    "print(nyc_station) \n",
    "print(nyc_station[\"LAT\"] + \",\" + nyc_station[\"LON\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "\n",
    "The code above should give us the following weather station:\n",
    "\n",
    "* USAF='725053'\n",
    "* WBAN='94728'\n",
    "* STATION NAME='CENTRAL PARK'\n",
    "* CTRY='US'\n",
    "* STATE='NY'\n",
    "* LAT='+40.779'\n",
    "* LON='-073.969'\n",
    "\n",
    "Please make sure to continue with these values, as the following code is tailored for specifically that weather station!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_station_usaf = \"725053\"\n",
    "nyc_station_wban = \"94728\"\n",
    "nyc_station_latitude = 40.779\n",
    "nyc_station_longitude = -73.969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Let's make a picture again, showing the average geo coordinate of our data and the weather station. They should match pretty well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_min_x=-8239719.95065924\n",
    "geo_max_x=-8212678.623952549\n",
    "geo_min_y=4968029.278728969\n",
    "geo_max_y=4989775.66725539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'LAT'  :[avg_pickup_latitude, nyc_station_latitude],\n",
    "    'LONG' :[avg_pickup_longitude, nyc_station_longitude]\n",
    "})\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame  \n",
    "coords = pd.Series(zip(df[\"LONG\"], df[\"LAT\"]))\n",
    "geo_df = gpd.GeoDataFrame(df, crs = {'init': 'epsg:4326'}, geometry = coords.apply(Point)).to_crs(epsg=3857)\n",
    "\n",
    "# ... and make the plot\n",
    "ax = geo_df.plot(figsize=(15, 10), alpha=1, color=\"red\")\n",
    "ax.set(ylim=(geo_min_y, geo_max_y), xlim=(geo_min_x, geo_max_x))\n",
    "\n",
    "# Add basemap below\n",
    "add_basemap(ax, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Holidays\n",
    "\n",
    "The last data set that we want to integrate is the list of bank holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = spark.read.parquet(structured_basedir + \"/holidays\")\n",
    "holidays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Join Data\n",
    "\n",
    "Finally we join together all four data sets:\n",
    "* Preaggregated taxi trips\n",
    "* Hourly weather data\n",
    "* Daily weather data\n",
    "* Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC Weather\n",
    "\n",
    "We filter the weather data to the NYC weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_daily_weather = daily_weather.filter((daily_weather[\"usaf\"] == nyc_station_usaf) & (daily_weather[\"wban\"] == nyc_station_wban)).cache()\n",
    "nyc_hourly_weather = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Data Sets\n",
    "\n",
    "Now we carefully join all enrichment information to the preaggregated hourly taxi trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = # YOUR CODE HERE\n",
    "\n",
    "joined_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since writing all the joins is a little bit tedious and error prone work, this has already been prepared for you with all sources in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = taxi_aggregates \\\n",
    "    .join(f.broadcast(holidays), [\"date\"], how=\"leftOuter\") \\\n",
    "    .drop(holidays[\"date\"]) \\\n",
    "    .drop(holidays[\"id\"]) \\\n",
    "    .withColumnRenamed(\"description\", \"holiday_description\") \\\n",
    "    .join(f.broadcast(nyc_hourly_weather), [\"date\", \"hour\"], how=\"leftOuter\") \\\n",
    "    .withColumnRenamed(\"precipitation\", \"hourly_precipitation\") \\\n",
    "    .withColumnRenamed(\"wind_speed\", \"hourly_wind_speed\") \\\n",
    "    .withColumnRenamed(\"temperature\", \"hourly_temperature\") \\\n",
    "    .drop(nyc_hourly_weather[\"date\"])\\\n",
    "    .drop(nyc_hourly_weather[\"hour\"]) \\\n",
    "    .drop(nyc_hourly_weather[\"usaf\"])\\\n",
    "    .drop(nyc_hourly_weather[\"wban\"])\\\n",
    "    .join(f.broadcast(nyc_daily_weather), [\"date\"], how=\"leftOuter\") \\\n",
    "    .withColumnRenamed(\"precipitation\", \"daily_precipitation\") \\\n",
    "    .withColumnRenamed(\"wind_speed\", \"daily_wind_speed\") \\\n",
    "    .withColumnRenamed(\"temperature\", \"daily_temperature\") \\\n",
    "    .drop(nyc_daily_weather[\"date\"])\\\n",
    "    .drop(nyc_daily_weather[\"usaf\"])\\\n",
    "    .drop(nyc_daily_weather[\"wban\"])\\\n",
    "    .orderBy(\"date\", \"hour\") \\\n",
    "    .cache()\n",
    "\n",
    "all_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Integrated Zone\n",
    "\n",
    "The result will be written into the integrated zone into the sub directory `taxi-trips-hourly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.write.mode(\"overwrite\").parquet(integrated_basedir + \"/taxi-trips-hourly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = spark.read.parquet(integrated_basedir + \"/taxi-trips-hourly\")\n",
    "all_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average temperature and total amount per date. Sort the result by date\n",
    "daily = # YOUR CODE HERE\n",
    "\n",
    "# Convert to Pandas    \n",
    "pdf = daily.toPandas()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Plot fare amount\n",
    "ax1.plot(pdf[\"date\"],pdf[\"amount\"], color=\"red\")\n",
    "\n",
    "# Plot temperature\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(pdf[\"date\"],pdf[\"temperature\"], color=\"green\")\n",
    "\n",
    "# Plot legends\n",
    "plt.legend(handles=[\n",
    "    mpatches.Patch(color='red', label='Total Fare Amount per Day'),\n",
    "    mpatches.Patch(color='green', label='Average Temperature per Day')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.4 (Python 3.7)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
